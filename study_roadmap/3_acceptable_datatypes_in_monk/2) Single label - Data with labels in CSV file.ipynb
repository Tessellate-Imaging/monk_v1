{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "### 1. Understand what single label image classifications is\n",
    "        - One label per image\n",
    "\n",
    "### 2. Understand single label - csv type data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "## [0. Install](#0)\n",
    "\n",
    "\n",
    "## [1. Load train data - csv type](#1)\n",
    "\n",
    "\n",
    "## [2. Understand experiment data-summary](#2)\n",
    "\n",
    "\n",
    "## [3. Load val data along with train data](#3)\n",
    "\n",
    "\n",
    "## [4. Understand experiment data-summary](#4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "# Install Monk\n",
    "  \n",
    " - git clone https://github.com/Tessellate-Imaging/monk_v1.git\n",
    " \n",
    " - cd monk_v1/installation && pip install -r requirements_cu9.txt\n",
    "     - (Select the requirements file as per OS and CUDA version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'monk_v1'...\n",
      "remote: Enumerating objects: 11, done.\u001b[K\n",
      "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
      "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
      "remote: Total 1916 (delta 4), reused 10 (delta 4), pack-reused 1905\u001b[K\n",
      "Receiving objects: 100% (1916/1916), 69.95 MiB | 10.27 MiB/s, done.\n",
      "Resolving deltas: 100% (1024/1024), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Tessellate-Imaging/monk_v1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the requirements file as per OS and CUDA version\n",
    "!cd monk_v1/installation && pip install -r requirements_cu9.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monk\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"monk_v1/monk/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using mxnet-gluon backend \n",
    "from gluon_prototype import prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and managing experiments\n",
    "    - Provide project name\n",
    "    - Provide experiment name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mxnet Version: 1.5.1\n",
      "\n",
      "Experiment Details\n",
      "    Project: Dataset-Type\n",
      "    Experiment: exp-csv\n",
      "    Dir: /home/abhi/Desktop/Work/tess_tool/gui/v0.3/finetune_models/Organization/development/v5.0_blocks/study_roadmap/acceptable_datatypes_in_monk/workspace/Dataset-Type/exp-csv/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gtf = prototype(verbose=1);\n",
    "gtf.Prototype(\"Dataset-Type\", \"exp-csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This creates files and directories as per the following structure\n",
    "    \n",
    "    \n",
    "    workspace\n",
    "        |\n",
    "        |--------Dataset-Type \n",
    "                        |\n",
    "                        |\n",
    "                        |-----exp-foldered \n",
    "                                    |\n",
    "                                    |-----experiment-state.json\n",
    "                                    |\n",
    "                                    |-----output\n",
    "                                            |\n",
    "                                            |------logs (All training logs and graphs saved here)\n",
    "                                            |\n",
    "                                            |------models (all trained models saved here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "# Load train data - foldered type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick mode training\n",
    "\n",
    "    - Using Default Function\n",
    "        - dataset_path\n",
    "        - model_name\n",
    "        - num_epochs\n",
    "        \n",
    "        \n",
    "## Dataset folder structure\n",
    "\n",
    "    parent_directory\n",
    "        |\n",
    "        |\n",
    "        |------train (folder can be named anything)\n",
    "                |------img1.jpg\n",
    "                |------img2.jpg\n",
    "                |------.... (and so on)\n",
    "        |\n",
    "        |------train_labels.csv (file can be named anything)\n",
    "        \n",
    "        \n",
    "    train_labels.csv has 2 columns (column names could be anything)\n",
    "        - ID                 | Labels\n",
    "          img1.jpg           |  cat\n",
    "          img2.jpg           |  dog\n",
    "          img3.jpg           |  cat\n",
    "          img4.jpg           |  dog\n",
    "                ..... (and so on)\n",
    "                \n",
    "### In similar fashion you can structure your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Along with providing dataset_path you now provide path_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Details\n",
      "    Train path:     monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train\n",
      "    Val path:       None\n",
      "    CSV train path: monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train.csv\n",
      "    CSV val path:   None\n",
      "\n",
      "Dataset Params\n",
      "    Input Size:   224\n",
      "    Batch Size:   4\n",
      "    Data Shuffle: True\n",
      "    Processors:   4\n",
      "    Train-val split:   0.7\n",
      "    Delimiter:   ,\n",
      "\n",
      "Pre-Composed Train Transforms\n",
      "[{'RandomHorizontalFlip': {'p': 0.8}}, {'Normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}]\n",
      "\n",
      "Pre-Composed Val Transforms\n",
      "[{'RandomHorizontalFlip': {'p': 0.8}}, {'Normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}]\n",
      "\n",
      "Dataset Numbers\n",
      "    Num train images: 70\n",
      "    Num val images:   30\n",
      "    Num classes:      2\n",
      "\n",
      "Model Params\n",
      "    Model name:           resnet18_v1\n",
      "    Use Gpu:              True\n",
      "    Use pretrained:       True\n",
      "    Freeze base network:  True\n",
      "\n",
      "Model Details\n",
      "    Loading pretrained model\n",
      "    Model Loaded on device\n",
      "        Model name:                           resnet18_v1\n",
      "        Num of potentially trainable layers:  61\n",
      "        Num of actual trainable layers:       1\n",
      "\n",
      "Optimizer\n",
      "    Name:          sgd\n",
      "    Learning rate: 0.01\n",
      "    Params:        {'lr': 0.01, 'momentum': 0, 'weight_decay': 0, 'momentum_dampening_rate': 0, 'clipnorm': 0.0, 'clipvalue': 0.0}\n",
      "\n",
      "\n",
      "\n",
      "Learning rate scheduler\n",
      "    Name:   steplr\n",
      "    Params: {'step_size': 1, 'gamma': 0.98, 'last_epoch': -1}\n",
      "\n",
      "Loss\n",
      "    Name:          softmaxcrossentropy\n",
      "    Params:        {'weight': None, 'batch_axis': 0, 'axis_to_sum_over': -1, 'label_as_categories': True, 'label_smoothing': False}\n",
      "\n",
      "Training params\n",
      "    Num Epochs: 5\n",
      "\n",
      "Display params\n",
      "    Display progress:          True\n",
      "    Display progress realtime: True\n",
      "    Save Training logs:        True\n",
      "    Save Intermediate models:  True\n",
      "    Intermediate model prefix: intermediate_model_\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monk_v1/monk/system/imports.py:193: UserWarning: ArgumentWarning: clipnorm and clipvalue are active only for keras in current version of Monk\n",
      "  warnings.warn(msg)\n",
      "monk_v1/monk/system/imports.py:193: UserWarning: ArgumentWarning: momentum_dampening_rate is active only for pytorch in current version of Monk\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gtf.Default(dataset_path=\"monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train\",\n",
    "            path_to_csv=\"monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train.csv\",\n",
    "            model_name=\"resnet18_v1\", \n",
    "            num_epochs=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "# Understand experiment data-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset Details\n",
    "        Train path:     monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train\n",
    "        Val path:       None\n",
    "        CSV train path: monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train.csv\n",
    "        CSV val path:   None\n",
    "        \n",
    "        \n",
    "### Train path is mentioned and Val path is None\n",
    "\n",
    "### CSV Train path is mentioned unlike in foldered type dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset Parameters:\n",
    "        Input Size:   224\n",
    "        Batch Size:   4\n",
    "        Shuffle:      True\n",
    "        Processors:   4\n",
    "        Num Classes:  2\n",
    "\n",
    "###  Data input size is 224 (default setting)\n",
    "###  Batch size is 4 (default setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset Numbers\n",
    "        Num train images: 70\n",
    "        Num val images:   30\n",
    "        Num classes:      2\n",
    "        \n",
    "### Since val data is missing, train data is split into train and val with a ratio 70:30 rule\n",
    "    - 70% of data being used for training\n",
    "    - 30% of data being used for validation\n",
    "    \n",
    "### Auto detect number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "# Load val data along with train data - foldered type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick mode training\n",
    "\n",
    "    - Using Default Function\n",
    "        - dataset_path\n",
    "        - model_name\n",
    "        - num_epochs\n",
    "        \n",
    "        \n",
    "## Dataset folder structure\n",
    "\n",
    "    parent_directory\n",
    "        |\n",
    "        |\n",
    "        |------train (folder can be named anything)\n",
    "                |------img1.jpg\n",
    "                |------img2.jpg\n",
    "                |------.... (and so on)\n",
    "        |\n",
    "        |------train_labels.csv (file can be named anything)\n",
    "        \n",
    "        \n",
    "    train_labels.csv has 2 columns (column names could be anything)\n",
    "        - ID                 | Labels\n",
    "          img1.jpg           |  cat\n",
    "          img2.jpg           |  dog\n",
    "          img3.jpg           |  cat\n",
    "          img4.jpg           |  dog\n",
    "                ..... (and so on)\n",
    "                \n",
    "                \n",
    "    parent_directory\n",
    "        |\n",
    "        |\n",
    "        |------val (folder can be named anything)\n",
    "                |------img1.jpg\n",
    "                |------img2.jpg\n",
    "                |------.... (and so on)\n",
    "        |\n",
    "        |------val_labels.csv (file can be named anything)\n",
    "        \n",
    "        \n",
    "    val_labels.csv has 2 columns (column names could be anything)\n",
    "        - ID                 | Labels\n",
    "          img1.jpg           |  cat\n",
    "          img2.jpg           |  dog\n",
    "          img3.jpg           |  cat\n",
    "          img4.jpg           |  dog\n",
    "                ..... (and so on)\n",
    "                \n",
    "                \n",
    "### In similar fashion you can structure your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instead of providing dataset as string, provide list of strings\n",
    "        - First string: Train data path\n",
    "        - Second string: Validation data path\n",
    "        \n",
    "        \n",
    "### Alongside the same goes with path_to_csv\n",
    "        - First string: Train data csv path\n",
    "        - Second string: Validation data csv path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Details\n",
      "    Train path:     monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train\n",
      "    Val path:       monk_v1/monk/system_check_tests/datasets/dataset_csv_id/val\n",
      "    CSV train path: monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train.csv\n",
      "    CSV val path:   monk_v1/monk/system_check_tests/datasets/dataset_csv_id/val.csv\n",
      "\n",
      "Dataset Params\n",
      "    Input Size:   224\n",
      "    Batch Size:   4\n",
      "    Data Shuffle: True\n",
      "    Processors:   4\n",
      "    Delimiter:   ,\n",
      "\n",
      "Pre-Composed Train Transforms\n",
      "[{'RandomHorizontalFlip': {'p': 0.8}}, {'Normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, {'RandomHorizontalFlip': {'p': 0.8}}, {'Normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}]\n",
      "\n",
      "Pre-Composed Val Transforms\n",
      "[{'RandomHorizontalFlip': {'p': 0.8}}, {'Normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}, {'RandomHorizontalFlip': {'p': 0.8}}, {'Normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}]\n",
      "\n",
      "Dataset Numbers\n",
      "    Num train images: 100\n",
      "    Num val images:   50\n",
      "    Num classes:      2\n",
      "\n",
      "Model Params\n",
      "    Model name:           resnet18_v1\n",
      "    Use Gpu:              True\n",
      "    Use pretrained:       True\n",
      "    Freeze base network:  True\n",
      "\n",
      "Model Details\n",
      "    Loading pretrained model\n",
      "    Model Loaded on device\n",
      "        Model name:                           resnet18_v1\n",
      "        Num of potentially trainable layers:  61\n",
      "        Num of actual trainable layers:       1\n",
      "\n",
      "Optimizer\n",
      "    Name:          sgd\n",
      "    Learning rate: 0.01\n",
      "    Params:        {'lr': 0.01, 'momentum': 0, 'weight_decay': 0, 'momentum_dampening_rate': 0, 'clipnorm': 0.0, 'clipvalue': 0.0}\n",
      "\n",
      "\n",
      "\n",
      "Learning rate scheduler\n",
      "    Name:   steplr\n",
      "    Params: {'step_size': 1, 'gamma': 0.98, 'last_epoch': -1}\n",
      "\n",
      "Loss\n",
      "    Name:          softmaxcrossentropy\n",
      "    Params:        {'weight': None, 'batch_axis': 0, 'axis_to_sum_over': -1, 'label_as_categories': True, 'label_smoothing': False}\n",
      "\n",
      "Training params\n",
      "    Num Epochs: 5\n",
      "\n",
      "Display params\n",
      "    Display progress:          True\n",
      "    Display progress realtime: True\n",
      "    Save Training logs:        True\n",
      "    Save Intermediate models:  True\n",
      "    Intermediate model prefix: intermediate_model_\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monk_v1/monk/system/imports.py:193: UserWarning: ArgumentWarning: clipnorm and clipvalue are active only for keras in current version of Monk\n",
      "  warnings.warn(msg)\n",
      "monk_v1/monk/system/imports.py:193: UserWarning: ArgumentWarning: momentum_dampening_rate is active only for pytorch in current version of Monk\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "gtf.Default(dataset_path=[\"monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train\",\n",
    "                          \"monk_v1/monk/system_check_tests/datasets/dataset_csv_id/val\"],\n",
    "            path_to_csv=[\"monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train.csv\",\n",
    "                         \"monk_v1/monk/system_check_tests/datasets/dataset_csv_id/val.csv\"],\n",
    "            model_name=\"resnet18_v1\", \n",
    "            num_epochs=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "# Understand experiment data-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset Details\n",
    "        Train path:     monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train\n",
    "        Val path:       monk_v1/monk/system_check_tests/datasets/dataset_csv_id/val\n",
    "        CSV train path: monk_v1/monk/system_check_tests/datasets/dataset_csv_id/train.csv\n",
    "        CSV val path:   monk_v1/monk/system_check_tests/datasets/dataset_csv_id/val.csv\n",
    "        \n",
    "        \n",
    "### Both Train and Val path are mentioned\n",
    "\n",
    "### Both CSV Train and val path are mentioned unlike in foldered type dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset Parameters:\n",
    "        Input Size:   224\n",
    "        Batch Size:   4\n",
    "        Shuffle:      True\n",
    "        Processors:   4\n",
    "        Num Classes:  2\n",
    "\n",
    "###  Data input size is 224 (default setting)\n",
    "###  Batch size is 4 (default setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Dataset Numbers\n",
    "        Num train images: 100\n",
    "        Num val images:   50\n",
    "        Num classes:      2\n",
    "        \n",
    "### Since val data is present, train data is not split further\n",
    "    - all 100 images of data being used for training\n",
    "    - all 50 images of data being used for validation\n",
    "    \n",
    "### Auto detect number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
